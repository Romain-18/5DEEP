{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaJ4euzPThpd"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Imports\n",
        "# =========================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import os, numpy as np\n",
        "from sklearn.model_selection import GroupShuffleSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvPgFeHtTjOF"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Montage Drive + chemins\n",
        "# =========================\n",
        "drive.mount(\"/content/drive\")\n",
        "BASE = \"/content/drive/MyDrive/IA/bird_songs_dataset\"\n",
        "CSV  = \"/content/drive/MyDrive/IA/bird_songs_metadata.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njUGUfMxTlx-"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Chargement CDV\n",
        "# =========================\n",
        "label_df = pd.read_csv(CSV)\n",
        "print(\"Aperçu des colonnes:\", list(label_df.columns))\n",
        "\n",
        "if \"filepath\" not in label_df.columns:\n",
        "    if \"filename\" in label_df.columns:\n",
        "        label_df[\"filepath\"] = label_df[\"filename\"].apply(lambda fn: os.path.join(BASE, str(fn)))\n",
        "    else:\n",
        "        raise ValueError(\"Aucune colonne 'filename' ou 'filepath' trouvée dans le CSV.\")\n",
        "\n",
        "label_df[\"exists\"] = label_df[\"filepath\"].apply(os.path.exists)\n",
        "missing = label_df[~label_df[\"exists\"]]\n",
        "print(\"Nb de fichiers manquants:\", len(missing))\n",
        "if len(missing):\n",
        "    print(\"Exemples de fichiers manquants :\")\n",
        "    print(missing.head())\n",
        "    label_df = label_df[label_df[\"exists\"]].copy()\n",
        "\n",
        "print(\"Nb de lignes après filtrage:\", len(label_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J1bQ1v_pAX1"
      },
      "source": [
        "### Définitions :\n",
        "\n",
        "**Amplitude** :\n",
        "Valeur instantanée du signal audio (proportionnelle à la pression acoustique). Elle détermine l’énergie/le volume perçu.\n",
        "\n",
        "**Fréquence** :\n",
        "Nombre d’oscillations par seconde (Hz). Les oiseaux chantent dans des bandes de fréquences caractéristiques.\n",
        "\n",
        "**Nombre de canaux** :\n",
        "Mono (1 canal) ou stéréo (2 canaux). Permet une spacialisation du son (2 canaux et +).\n",
        "\n",
        "**Fréquence d’échantillonnage (SR)** :\n",
        "Nombre d’échantillons par seconde (Hz). Généralement plus elle est haute plus la qualité est bonne."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVEWAbqfuN91"
      },
      "source": [
        "### Format WAV et en-têtes\n",
        "\n",
        "Un fichier **WAV** est découpé en plusieurs blocs :\n",
        "\n",
        "- RIFF : identifiant + taille totale du fichier.\n",
        "- WAVE : signature indiquant le type de données audio.\n",
        "- fmt : informations de format (fréquence d’échantillonnage, nombre de canaux, profondeur en bits).\n",
        "- data : bloc contenant les échantillons audio bruts.\n",
        "\n",
        "Exemple : un WAV 44,1 kHz / 16 bits / 2 canaux aura dans son header :\n",
        "- Sample rate = 44100 Hz\n",
        "- Bits per sample = 16\n",
        "- Channels = 2\n",
        "- Byte rate = 44100 × 2 (stéréo) × 16/8 = 176,4 kB/s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQEbiWG6TpJF"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Exploration basique + enrichissement CSV\n",
        "# =========================\n",
        "durations, orig_srs, n_channels = [], [], []\n",
        "rms_vals, centroids, bandwidths = [], [], []\n",
        "\n",
        "for filepath in tqdm(label_df[\"filepath\"], desc=\"Scan audio (durée/sr/canaux + rms/ctr/bdw)\"):\n",
        "    y, sr = librosa.load(filepath, sr=None, mono=False)\n",
        "\n",
        "    # canaux / durée\n",
        "    if y.ndim == 1:\n",
        "        channels = 1\n",
        "        duration = len(y) / sr\n",
        "    else:\n",
        "        channels = y.shape[0]\n",
        "        duration = y.shape[1] / sr\n",
        "\n",
        "    # passage en mono pour features spectrales\n",
        "    y_mono = y if y.ndim == 1 else np.mean(y, axis=0)\n",
        "\n",
        "    # features rms - ctr - bdw\n",
        "    try:\n",
        "        rms = float(librosa.feature.rms(y=y_mono).mean())\n",
        "    except Exception:\n",
        "        rms = np.nan\n",
        "    try:\n",
        "        centroid = float(librosa.feature.spectral_centroid(y=y_mono, sr=sr).mean())\n",
        "    except Exception:\n",
        "        centroid = np.nan\n",
        "    try:\n",
        "        bw = float(librosa.feature.spectral_bandwidth(y=y_mono, sr=sr).mean())\n",
        "    except Exception:\n",
        "        bw = np.nan\n",
        "\n",
        "    durations.append(duration)\n",
        "    orig_srs.append(sr)\n",
        "    n_channels.append(channels)\n",
        "    rms_vals.append(rms)\n",
        "    centroids.append(centroid)\n",
        "    bandwidths.append(bw)\n",
        "\n",
        "# Ajout colonnes enrichies\n",
        "label_df[\"duration_s\"]         = durations\n",
        "label_df[\"orig_sr\"]            = orig_srs\n",
        "label_df[\"n_channels\"]         = n_channels\n",
        "label_df[\"rms_energy\"]         = rms_vals\n",
        "label_df[\"spectral_centroid\"]  = centroids\n",
        "label_df[\"spectral_bandwidth\"] = bandwidths\n",
        "\n",
        "print(label_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCb-E1yhpH55"
      },
      "source": [
        "* **duration_s** = durée totale du fichier audio.\n",
        "\n",
        "* **orig_sr** = fréquence d’échantillonnage d’origine.\n",
        "\n",
        "* **n_channels** = nombre de canaux.\n",
        "\n",
        "* **rms_energy** = : énergie racine carrée moyenne du signal. Elle reflète l’intensité/volume global d’un son. Valeurs hautes = son fort, basses = son faible.\n",
        "\n",
        "* **spectral_centroid** = barycentre du spectre de fréquences. Souvent perçu comme une mesure de la “brillance” d’un son (plus le centroid est haut, plus le son paraît aigu/clair).\n",
        "\n",
        "* **spectral_bandwidth** = largeur effective du spectre autour du centroid. Indique la dispersion des fréquences (sons purs = bande étroite, sons riches/bruités = bande large)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v7MS_LqTtbe"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Visualisation simple\n",
        "# =========================\n",
        "\n",
        "# 1) Un exemple d'onde par classe\n",
        "classes = label_df[\"species\"].unique()\n",
        "plt.figure(figsize=(12, max(2.5, 2.5 * len(classes))))\n",
        "for idx, cls in enumerate(classes):\n",
        "    row = label_df[label_df[\"species\"] == cls].sample(1, random_state=42).iloc[0]\n",
        "    filepath = row[\"filepath\"]\n",
        "    y, sr = librosa.load(filepath, sr=22050, mono=True)\n",
        "    t = np.arange(len(y)) / sr\n",
        "\n",
        "    plt.subplot(len(classes), 1, idx + 1)\n",
        "    plt.plot(t, y)\n",
        "    plt.title(f\"{cls} — {os.path.basename(filepath)}\", fontsize=10)\n",
        "    plt.xlabel(\"Temps (s)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) Répartition des classes\n",
        "class_counts = label_df[\"species\"].value_counts().sort_index()\n",
        "print(\"\\nNombre de sons par classe:\\n\", class_counts)\n",
        "plt.figure(figsize=(8, 5))\n",
        "class_counts.plot(kind=\"bar\", edgecolor=\"black\")\n",
        "plt.title(\"Répartition des sons par classe\")\n",
        "plt.xlabel(\"Classe d'oiseau\")\n",
        "plt.ylabel(\"Nombre de fichiers\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Distribution du nombre de canaux\n",
        "plt.figure(figsize=(4,3))\n",
        "label_df[\"n_channels\"].value_counts().plot.bar(color=\"skyblue\")\n",
        "plt.title(\"Distribution du nombre de canaux\")\n",
        "plt.xlabel(\"Nb canaux\")\n",
        "plt.ylabel(\"Comptage\")\n",
        "plt.show()\n",
        "\n",
        "# 4) Distribution de la fréquence d'échantillonnage d'origine\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(label_df[\"orig_sr\"], bins=20, kde=True, color=\"orange\")\n",
        "plt.title(\"Distribution des fréquences d'échantillonnage originales\")\n",
        "plt.xlabel(\"Fréquence (Hz)\")\n",
        "plt.ylabel(\"Comptage\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 5) Distribution durées\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(label_df[\"duration_s\"], bins=50, kde=True)\n",
        "plt.title(\"Distribution des durées des enregistrements\")\n",
        "plt.xlabel(\"Durée (secondes)\")\n",
        "plt.ylabel(\"Nombre de fichiers\")\n",
        "plt.show()\n",
        "\n",
        "# Sauvegarde CSV enrichi\n",
        "OUT = \"/content/drive/MyDrive/IA/bird_songs_metadata_enriched.csv\"\n",
        "label_df.drop(columns=[\"exists\"], errors=\"ignore\").to_csv(OUT, index=False)\n",
        "print(f\"CSV enrichi sauvegardé : {OUT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqWSdVLppgeg"
      },
      "source": [
        "# Pourquoi le tracé temps-amplitude ne suffit pas ?\n",
        "\n",
        "La forme d’onde montre quand le signal est fort/faible, mais presque rien sur où se situe l’énergie en fréquence ou encore le timbre. Deux chants peuvent avoir des amplitudes similaires et des contenus fréquentiels très différents. C'est pour cela que nous allons utiliser le Mel-spectrogram, logmel spectrograme ou encore le MFCC pour récupérer des informations plus adaptées à la classification.\n",
        "\n",
        "> Ajouter une citation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DqCTGxLXdG4"
      },
      "source": [
        "* Tout les enregistrements font 22050 Hz\n",
        "* Tout les enregistrements font 3 secondes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zviSLguapvq8"
      },
      "source": [
        "L’écart est modéré (minimum à 890 et maximum à 1 250, écart max à 360).\n",
        "\n",
        "-> Pas obligatoire d’équilibrer, on surveillera les metrics pour confirmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT4Ec4wlTzcu"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Encodage des labels\n",
        "# =========================\n",
        "le = LabelEncoder()\n",
        "y_all = le.fit_transform(label_df[\"species\"].values)\n",
        "y_all_ref = y_all.copy()\n",
        "print(\"Classes:\", le.classes_)\n",
        "print(\"y_all shape:\", y_all_ref.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzwSrXAhT0mI"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Paramètres audio globaux\n",
        "# =========================\n",
        "SR      = 22050\n",
        "N_FFT   = 2048\n",
        "HOP     = 512\n",
        "N_MFCC  = 40\n",
        "N_MELS  = 128\n",
        "\n",
        "TARGET_DUR = 3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LILHa_JqBFM"
      },
      "source": [
        "# Définitions :\n",
        "\n",
        "**Mel-spectrogram** : on calcule un spectrogramme par STFT  puis on projette l’énergie sur une banque de filtres Mel (représentation quasi-log de la hauteur perçue). On prend souvent le log de l’énergie (log-Mel) pour rapprocher la dynamique de l’oreille humaine.\n",
        "\n",
        "**MFCC** : on applique une DCT (transformée en cosinus) sur le log-Mel pour décorréler les bandes et compacter l’info dans les premiers N coefficients (ici 40). Les MFCCs résument le timbre ; on peut ajouter Δ et ΔΔ (dérivées 1ère/2ème) pour capturer la dynamique temporelle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEfw9iE0T2-g"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Utilitaires audio/features\n",
        "# =========================\n",
        "def load_fixed_mono(path, sr=SR, target_dur=TARGET_DUR):\n",
        "    y, _ = librosa.load(path, sr=sr, mono=True)\n",
        "    target_len = int(sr * target_dur)\n",
        "    if len(y) < target_len:\n",
        "        pad = target_len - len(y)\n",
        "        y = np.pad(y, (0, pad), mode=\"constant\")\n",
        "    else:\n",
        "        y = y[:target_len]\n",
        "    return y\n",
        "\n",
        "def mfcc_flat_from_path(path):\n",
        "    \"\"\"MFCC 2D -> vecteur 1D (pour MLP).\"\"\"\n",
        "    y = load_fixed_mono(path)\n",
        "    mf = librosa.feature.mfcc(y=y, sr=SR, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP)\n",
        "    mf = (mf - mf.mean(axis=1, keepdims=True)) / (mf.std(axis=1, keepdims=True) + 1e-8) # Si mf = matrice (40, 200), alors mf = (40, 1).\n",
        "    return mf.flatten().astype(np.float32)\n",
        "\n",
        "def mfcc_2d_from_path(path):\n",
        "    \"\"\"MFCC 2D normalisé, shape (N_MFCC, T, 1).\"\"\"\n",
        "    y = load_fixed_mono(path)\n",
        "    mf = librosa.feature.mfcc(y=y, sr=SR, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP).astype(np.float32)\n",
        "    mf = (mf - mf.mean(axis=1, keepdims=True)) / (mf.std(axis=1, keepdims=True) + 1e-8)\n",
        "    return mf[..., np.newaxis]\n",
        "\n",
        "def logmel_2d(y, sr=SR, n_fft=N_FFT, hop=HOP, n_mels=N_MELS, norm=\"per_band\"):\n",
        "    \"\"\"Calcule un log-Mel 2D (N_MELS x T), normalisé.\"\"\"\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop, n_mels=n_mels)\n",
        "    logmel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "    if norm == \"per_band\":\n",
        "        mu  = logmel.mean(axis=1, keepdims=True)\n",
        "        std = logmel.std(axis=1, keepdims=True) + 1e-8\n",
        "        logmel = (logmel - mu) / std\n",
        "    elif norm == \"global\":\n",
        "        logmel = (logmel - logmel.mean()) / (logmel.std() + 1e-8)\n",
        "\n",
        "    return logmel.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Onde, STFT (dB), Mel, log-Mel, MFCC\n",
        "# ============================================\n",
        "\n",
        "row = label_df.sample(1, random_state=42).iloc[0]\n",
        "fp = row[\"filepath\"]\n",
        "species = row.get(\"species\", \"unknown\")\n",
        "print(\"Fichier choisi:\", fp, \"| espèce:\", species)\n",
        "\n",
        "y = load_fixed_mono(fp, sr=SR, target_dur=TARGET_DUR)\n",
        "\n",
        "# Onde temporelle\n",
        "plt.figure(figsize=(10, 3))\n",
        "t = np.arange(len(y)) / SR\n",
        "plt.plot(t, y)\n",
        "plt.title(f\"Onde temporelle — {species}\")\n",
        "plt.xlabel(\"Temps (s)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Spectrogramme STFT\n",
        "S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP))**2\n",
        "S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(S_db, sr=SR, hop_length=HOP, x_axis=\"time\", y_axis=\"hz\")\n",
        "plt.title(f\"Spectrogramme STFT (dB) — {species}\")\n",
        "plt.colorbar(format=\"%+0.1f dB\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Mel-spectrogram\n",
        "M = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, n_mels=N_MELS)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(M, sr=SR, hop_length=HOP, x_axis=\"time\", y_axis=\"mel\")\n",
        "plt.title(f\"Mel-spectrogram (puissance) — {species}\")\n",
        "plt.colorbar()\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# log-Mel\n",
        "M_db = librosa.power_to_db(M, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(M_db, sr=SR, hop_length=HOP, x_axis=\"time\", y_axis=\"mel\")\n",
        "plt.title(f\"log-Mel (dB) — {species}\")\n",
        "plt.colorbar(format=\"%+0.1f dB\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# MFCC\n",
        "mfcc = librosa.feature.mfcc(S=M_db, n_mfcc=N_MFCC)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfcc, sr=SR, hop_length=HOP, x_axis=\"time\")\n",
        "plt.title(f\"MFCC (N={N_MFCC}) — {species}\")\n",
        "plt.colorbar()\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "o2WVyPvdTExn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO8slckgT4m1"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Helpers plots\n",
        "# =========================\n",
        "def plot_history(history, title_prefix=\"\"):\n",
        "    plt.figure()\n",
        "    plt.plot(history.history[\"accuracy\"])\n",
        "    plt.plot(history.history[\"val_accuracy\"])\n",
        "    plt.title(f\"{title_prefix} Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Acc\")\n",
        "    plt.legend([\"train\", \"val\"])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history[\"loss\"])\n",
        "    plt.plot(history.history[\"val_loss\"])\n",
        "    plt.title(f\"{title_prefix} Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([\"train\", \"val\"])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion(y_true, y_pred, labels=None, title=\"Matrice de confusion (normalisée)\"):\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    if labels is None:\n",
        "        labels = np.arange(len(np.unique(y_true)))\n",
        "    ax.set_xticks(np.arange(len(labels))); ax.set_yticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels(labels, rotation=45, ha=\"right\"); ax.set_yticklabels(labels)\n",
        "    ax.set_xlabel(\"Prédit\"); ax.set_ylabel(\"Vrai\")\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le jeu de données contient plusieurs extraits issus d’un même enregistrement source, exemple :\n",
        "* 557838-0.wav   \n",
        "* 557838-1.wav   \n",
        "* 557838-4.wav   \n",
        "\n",
        "Si on fait un split aléatoir, des extraits quasi identiques (même oiseau, même lieu, même bruit de fond, même micro) peuvent se retrouver à la fois dans l’entraînement et dans la validation/test. Le modèle “reconnaît” alors des signatures parasites plutôt que des caractéristiques vraiment discriminantes de l’espèce ce qui provoque des scores artificiellement gonflés.\n",
        "\n",
        "\n",
        "On impose donc un split par groupe d’enregistrement : on dérive un record_id à partir du nom de fichier afin de regrouper tous les extraits du même enregistrement.\n",
        "\n",
        "Puis on utilise GroupShuffleSplit pour séparer train / val / test en veillant à ce qu’aucun record_id ne soit partagé entre les splits. Ainsi, un enregistrement complet n’apparaît que dans un seul ensemble, ce qui supprime la fuite d’information."
      ],
      "metadata": {
        "id": "m-YVIBINLS7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKyrbzPBWUBT"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Anti Leak\n",
        "# ============================================================\n",
        "\n",
        "if \"filename\" not in label_df.columns:\n",
        "    label_df[\"filename\"] = label_df[\"filepath\"].apply(lambda p: os.path.basename(p))\n",
        "def _recid(fn: str) -> str:\n",
        "    base = os.path.splitext(fn)[0]\n",
        "    return base.split(\"-\")[0].split(\"_\")[0]\n",
        "label_df[\"record_id\"] = label_df[\"filename\"].apply(_recid)\n",
        "\n",
        "files_all = label_df[\"filepath\"].values\n",
        "y_all     = np.asarray(y_all_ref, dtype=int)\n",
        "groups    = label_df[\"record_id\"].values\n",
        "\n",
        "gss1 = GroupShuffleSplit(test_size=0.30, n_splits=1, random_state=42)\n",
        "idx_train, idx_temp = next(gss1.split(files_all, y_all, groups=groups))\n",
        "gss2 = GroupShuffleSplit(test_size=0.50, n_splits=1, random_state=42)\n",
        "idx_val_rel, idx_test_rel = next(gss2.split(files_all[idx_temp], y_all[idx_temp], groups=groups[idx_temp]))\n",
        "idx_val_abs  = idx_temp[idx_val_rel]\n",
        "idx_test_abs = idx_temp[idx_test_rel]\n",
        "\n",
        "def slice_split(X, y):\n",
        "    return X[idx_train], X[idx_val_abs], X[idx_test_abs], y[idx_train], y[idx_val_abs], y[idx_test_abs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KF8u1qUqNgn"
      },
      "source": [
        "Réseau de neurones pleinement connecté (MLP)\n",
        "\n",
        "Le premier modèle implémenté est un réseau dense appliqué sur les MFCC vectorisés.\n",
        "Bien que ce modèle capture une certaine structure, il ne prend pas en compte les relations temporelles et fréquentielles.\n",
        "Le score obtenu est d’environ 46 % d’accuracy test (macro-F1 ≈ 0.45), ce qui montre une limite importante dans sa capacité à généraliser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHtA9sqJT8AF"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PIPELINE 1 — MLP sur MFCC flatten (entrée 1D)\n",
        "# ============================================================\n",
        "\n",
        "# Extraction MFCC 1D\n",
        "X_list, y_mlp_list = [], []\n",
        "\n",
        "for fp, yi in tqdm(zip(label_df[\"filepath\"].values, y_all), total=len(label_df), desc=\"MFCC flatten (MLP)\"):\n",
        "    try:\n",
        "        X_list.append(mfcc_flat_from_path(fp))   # -> vecteur 1D\n",
        "        y_mlp_list.append(yi)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {fp} -> {e}\")\n",
        "\n",
        "# passage en matrice\n",
        "X_mlp = np.vstack(X_list)\n",
        "y_mlp = np.array(y_mlp_list, dtype=int)\n",
        "print(\"MLP — Features:\", X_mlp.shape, \"| Labels:\", y_mlp.shape)\n",
        "\n",
        "\n",
        "df_mfcc = pd.DataFrame(X_mlp, columns=[f\"mfcc_{i}\" for i in range(X_mlp.shape[1])])\n",
        "df_mfcc[\"species\"] = [le.inverse_transform([y])[0] for y in y_mlp]\n",
        "\n",
        "display(df_mfcc.head())\n",
        "print(\"Shape du DataFrame MFCC+label :\", df_mfcc.shape)\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = slice_split(X_mlp, y_mlp)\n",
        "\n",
        "# Encodage (one-hot encoded)\n",
        "num_classes_mlp = len(np.unique(y_mlp))\n",
        "y_train_oh = to_categorical(y_train, num_classes_mlp)\n",
        "y_val_oh   = to_categorical(y_val,   num_classes_mlp)\n",
        "y_test_oh  = to_categorical(y_test,  num_classes_mlp)\n",
        "\n",
        "model_mlp = keras.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes_mlp, activation=\"softmax\"),\n",
        "])\n",
        "model_mlp.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "cbs_mlp = [\n",
        "    callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(patience=3, factor=0.5),\n",
        "]\n",
        "history_mlp = model_mlp.fit(\n",
        "    X_train, y_train_oh,\n",
        "    validation_data=(X_val, y_val_oh),\n",
        "    epochs=40, batch_size=64, callbacks=cbs_mlp, verbose=1\n",
        ")\n",
        "\n",
        "# Scores\n",
        "y_pred_test_mlp = model_mlp.predict(X_test, batch_size=64).argmax(axis=1)\n",
        "print(\"\\n[MLP] Test accuracy:\", (y_pred_test_mlp == y_test).mean())\n",
        "print(\"[MLP] Macro-F1:\", f1_score(y_test, y_pred_test_mlp, average=\"macro\"))\n",
        "print(classification_report(y_test, y_pred_test_mlp, target_names=le.classes_))\n",
        "\n",
        "plot_history(history_mlp, title_prefix=\"[MLP]\")\n",
        "plot_confusion(y_test, y_pred_test_mlp, labels=le.classes_, title=\"[MLP] Matrice de confusion (normalisée)\")\n",
        "\n",
        "model_mlp.save(\"/content/drive/MyDrive/IA/Model Save/Model_MFCC_1D.keras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_0HdoCwqRMK"
      },
      "source": [
        "Réseau de neurones convolutif (CNN, MFCC 2D)\n",
        "\n",
        "Le deuxième modèle, un CNN appliqué sur les MFCC 2D, exploite directement la structure temps-fréquence du signal.\n",
        "Cette approche permet d’apprendre des motifs locaux caractéristiques des espèces d’oiseaux.\n",
        "Le modèle atteint environ 74 % d’accuracy test et un macro-F1 ≈ 0.74, marquant un progrès significatif par rapport au MLP.\n",
        "L’analyse de la matrice de confusion montre une bonne reconnaissance de classes comme cardinalis (rappel = 0.93), mais certaines confusions persistent, notamment entre polyglottos et migratorius."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQWirT6eT_aL"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PIPELINE 2 — CNN sur MFCC 2D\n",
        "# ============================================================\n",
        "X2_list, y_mfcc2d_list = [], []\n",
        "for fp, yi in tqdm(zip(label_df[\"filepath\"].values, y_all_ref),\n",
        "                   total=len(label_df), desc=\"MFCC 2D (CNN)\"):\n",
        "    try:\n",
        "        X2_list.append(mfcc_2d_from_path(fp))\n",
        "        y_mfcc2d_list.append(yi)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {fp} -> {e}\")\n",
        "\n",
        "X_mfcc2d = np.stack(X2_list)  # (N, N_MFCC, T, 1)\n",
        "y_mfcc2d = np.array(y_mfcc2d_list, dtype=int)\n",
        "print(\"MFCC2D — Features:\", X_mfcc2d.shape, \"| Labels:\", y_mfcc2d.shape)\n",
        "\n",
        "X2_train, X2_val, X2_test, y2_train, y2_val, y2_test = slice_split(X_mfcc2d, y_mfcc2d)\n",
        "\n",
        "classes_unique = np.unique(y2_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_unique, y=y2_train)\n",
        "class_weight = {int(c): float(w) for c, w in zip(classes_unique, cw)}\n",
        "print(\"class_weight:\", class_weight)\n",
        "\n",
        "num_classes_cnn_mfcc = len(np.unique(y_mfcc2d))\n",
        "\n",
        "inp_mfcc = layers.Input(shape=(X2_train.shape[1], X2_train.shape[2], 1))\n",
        "x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(inp_mfcc)\n",
        "x = layers.BatchNormalization()(x); x = layers.MaxPooling2D((2,2))(x)\n",
        "x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x); x = layers.MaxPooling2D((2,2))(x)\n",
        "x = layers.Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x); x = layers.MaxPooling2D((2,2))(x)\n",
        "x = layers.Dropout(0.4)(x); x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x); x = layers.Dropout(0.4)(x)\n",
        "out_mfcc = layers.Dense(num_classes_cnn_mfcc, activation=\"softmax\")(x)\n",
        "\n",
        "model_cnn_mfcc = keras.Model(inp_mfcc, out_mfcc)\n",
        "model_cnn_mfcc.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                       loss=\"sparse_categorical_crossentropy\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "\n",
        "cbs_cnn_mfcc = [\n",
        "    callbacks.EarlyStopping(patience=6, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(patience=3, factor=0.5),\n",
        "]\n",
        "\n",
        "history_cnn_mfcc = model_cnn_mfcc.fit(\n",
        "    X2_train, y2_train,\n",
        "    validation_data=(X2_val, y2_val),\n",
        "    epochs=40, batch_size=64, callbacks=cbs_cnn_mfcc, verbose=1,\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "y2_pred = model_cnn_mfcc.predict(X2_test, batch_size=64).argmax(axis=1)\n",
        "print(\"\\n[CNN MFCC2D] Test accuracy:\", (y2_pred == y2_test).mean())\n",
        "print(\"[CNN MFCC2D] Macro-F1:\", f1_score(y2_test, y2_pred, average=\"macro\"))\n",
        "print(classification_report(y2_test, y2_pred, target_names=le.classes_))\n",
        "\n",
        "plot_history(history_cnn_mfcc, title_prefix=\"[CNN MFCC2D]\")\n",
        "plot_confusion(y2_test, y2_pred, labels=le.classes_, title=\"[CNN MFCC2D] Matrice de confusion (normalisée)\")\n",
        "\n",
        "model_cnn_mfcc.save(\"/content/drive/MyDrive/IA/Model Save/Model_MFCC_2D.keras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ozJkI7qUBV"
      },
      "source": [
        "Réseau de neurones convolutif (CNN, Log-Mel)\n",
        "\n",
        "Une troisième variante utilise les log-Mel spectrogrammes comme entrée, offrant une représentation plus proche de la perception humaine du son.\n",
        "Cette méthode conserve les avantages du CNN tout en exploitant la richesse fréquentielle du log-Mel.\n",
        "Le modèle obtient environ 69 % d’accuracy test (macro-F1 ≈ 0.70).\n",
        "Bien que légèrement inférieur au CNN-MFCC, il confirme la pertinence de cette représentation spectrale et ouvre la voie à des architectures hybrides plus puissantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytvc0pyQTfCx"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PIPELINE 3 — CNN sur Log-Mel\n",
        "# ============================================================\n",
        "Xl_list, yl_list = [], []\n",
        "for fp, yi in tqdm(zip(label_df[\"filepath\"].values, y_all_ref),\n",
        "                   total=len(label_df), desc=\"Log-Mel (CNN)\"):\n",
        "    try:\n",
        "        y_wav = load_fixed_mono(fp, sr=SR, target_dur=TARGET_DUR)\n",
        "        feat  = logmel_2d(y_wav, sr=SR, n_fft=N_FFT, hop=HOP, n_mels=N_MELS, norm=\"per_band\")\n",
        "        feat  = feat[..., np.newaxis]  # (N_MELS, T, 1)\n",
        "        Xl_list.append(feat)\n",
        "        yl_list.append(yi)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {fp} -> {e}\")\n",
        "\n",
        "X_logmel = np.stack(Xl_list)\n",
        "y_logmel = np.array(yl_list, dtype=int)\n",
        "print(\"LogMel — Features:\", X_logmel.shape, \"| Labels:\", y_logmel.shape)\n",
        "\n",
        "X_train_lm, X_val_lm, X_test_lm, y_train_lm, y_val_lm, y_test_lm = slice_split(X_logmel, y_logmel)\n",
        "num_classes_cnn_lm = len(np.unique(y_logmel))\n",
        "\n",
        "# Modèle CNN LogMel (même archi)\n",
        "inp_lm = layers.Input(shape=(X_train_lm.shape[1], X_train_lm.shape[2], 1))\n",
        "x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(inp_lm)\n",
        "x = layers.BatchNormalization()(x); x = layers.MaxPooling2D((2,2))(x)\n",
        "x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x); x = layers.MaxPooling2D((2,2))(x)\n",
        "x = layers.Conv2D(128, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x); x = layers.MaxPooling2D((2,2))(x)\n",
        "x = layers.Dropout(0.4)(x); x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x); x = layers.Dropout(0.4)(x)\n",
        "out_lm = layers.Dense(num_classes_cnn_lm, activation=\"softmax\")(x)\n",
        "\n",
        "model_cnn_logmel = keras.Model(inp_lm, out_lm)\n",
        "model_cnn_logmel.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                         loss=\"sparse_categorical_crossentropy\",\n",
        "                         metrics=[\"accuracy\"])\n",
        "cbs_cnn_lm = [\n",
        "    callbacks.EarlyStopping(patience=6, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(patience=3, factor=0.5),\n",
        "]\n",
        "history_cnn_logmel = model_cnn_logmel.fit(\n",
        "    X_train_lm, y_train_lm, validation_data=(X_val_lm, y_val_lm),\n",
        "    epochs=40, batch_size=64, callbacks=cbs_cnn_lm, verbose=1\n",
        ")\n",
        "\n",
        "y_pred_lm = model_cnn_logmel.predict(X_test_lm, batch_size=64).argmax(axis=1)\n",
        "print(\"\\n[CNN LogMel] Test accuracy:\", (y_pred_lm == y_test_lm).mean())\n",
        "print(\"[CNN LogMel] Macro-F1:\", f1_score(y_test_lm, y_pred_lm, average=\"macro\"))\n",
        "print(classification_report(y_test_lm, y_pred_lm, target_names=le.classes_))\n",
        "\n",
        "plot_history(history_cnn_logmel, title_prefix=\"[CNN LogMel]\")\n",
        "plot_confusion(y_test_lm, y_pred_lm, labels=le.classes_, title=\"[CNN LogMel] Matrice de confusion (normalisée)\")\n",
        "\n",
        "model_cnn_logmel.save(\"/content/drive/MyDrive/IA/Model Save/Model_CNN_LogMel.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réseau CRNN (CNN + BiGRU, Log-Mel)\n",
        "\n",
        "Le quatrième modèle combine des couches convolutives pour extraire des motifs locaux et un réseau récurrent bidirectionnel (BiGRU) pour modéliser la dynamique temporelle du chant.\n",
        "Cette architecture dite CRNN permet de capturer à la fois les structures fréquentielles et les dépendances temporelles à long terme.\n",
        "Elle atteint environ 78 % d’accuracy test et un macro-F1 ≈ 0.78, illustrant un net gain par rapport aux CNN purs.\n",
        "Les classes bewickii et migratorius sont particulièrement bien reconnues (rappel ≈ 0.80)."
      ],
      "metadata": {
        "id": "HDpXHA9LR1AB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-MbMj8302A"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PIPELINE 4 — CRNN (CNN + BiGRU) sur Log-Mel\n",
        "# ============================================================\n",
        "\n",
        "Xcr_list, ycr_list = [], []\n",
        "for fp, yi in tqdm(zip(label_df[\"filepath\"].values, y_all_ref),\n",
        "                   total=len(label_df), desc=\"Log-Mel (CRNN)\"):\n",
        "    try:\n",
        "        y_wav = load_fixed_mono(fp, sr=SR, target_dur=TARGET_DUR)\n",
        "        feat  = logmel_2d(y_wav, sr=SR, n_fft=N_FFT, hop=HOP, n_mels=N_MELS, norm=\"per_band\")\n",
        "        feat  = feat[..., np.newaxis]  # (F, T, 1)\n",
        "        Xcr_list.append(feat)\n",
        "        ycr_list.append(yi)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {fp} -> {e}\")\n",
        "\n",
        "X_crnn = np.stack(Xcr_list)\n",
        "y_crnn = np.array(ycr_list, dtype=int)\n",
        "print(\"CRNN — Features:\", X_crnn.shape, \"| Labels:\", y_crnn.shape)\n",
        "\n",
        "X_train_c, X_val_c, X_test_c, y_train_c, y_val_c, y_test_c = slice_split(X_crnn, y_crnn)\n",
        "\n",
        "# class_weight sur le TRAIN\n",
        "classes_unique = np.unique(y_train_c)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_unique, y=y_train_c)\n",
        "class_weight = {int(c): float(w) for c, w in zip(classes_unique, cw)}\n",
        "print(\"class_weight:\", class_weight)\n",
        "\n",
        "num_classes_crnn = len(np.unique(y_crnn))\n",
        "\n",
        "# Modèle CRNN (CNN → BiGRU → GlobalAveragePooling1D → Dense)\n",
        "def conv_block(x, filters, pool=(2,2), drop=0.0):\n",
        "    x = layers.Conv2D(filters, (3,3), padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D(pool)(x)\n",
        "    if drop and drop > 0:\n",
        "        x = layers.Dropout(drop)(x)\n",
        "    return x\n",
        "\n",
        "inp = layers.Input(shape=(X_train_c.shape[1], X_train_c.shape[2], 1))  # (F, T, 1)\n",
        "x = conv_block(inp,  32, pool=(2,2), drop=0.10)\n",
        "x = conv_block(x,    64, pool=(2,2), drop=0.10)\n",
        "x = conv_block(x,   128, pool=(2,1), drop=0.20)\n",
        "# (F', T', C) -> (T', F', C) -> (T', F'*C)\n",
        "x = layers.Permute((2,1,3))(x)\n",
        "x = layers.TimeDistributed(layers.Flatten())(x)\n",
        "# RNN bidirectionnel (GRU)\n",
        "x = layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "# Agrégation temporelle moyenne\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "out = layers.Dense(num_classes_crnn, activation=\"softmax\")(x)\n",
        "\n",
        "model_crnn = keras.Model(inp, out)\n",
        "model_crnn.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                   loss=\"sparse_categorical_crossentropy\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "\n",
        "cbs_crnn = [\n",
        "    callbacks.EarlyStopping(patience=6, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(patience=3, factor=0.5),\n",
        "]\n",
        "\n",
        "history_crnn = model_crnn.fit(\n",
        "    X_train_c, y_train_c,\n",
        "    validation_data=(X_val_c, y_val_c),\n",
        "    epochs=40, batch_size=64, callbacks=cbs_crnn, verbose=1,\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "# Scores\n",
        "y_pred_c = model_crnn.predict(X_test_c, batch_size=64).argmax(axis=1)\n",
        "print(\"\\n[CRNN] Test accuracy:\", (y_pred_c == y_test_c).mean())\n",
        "print(\"[CRNN] Macro-F1:\", f1_score(y_test_c, y_pred_c, average=\"macro\"))\n",
        "print(classification_report(y_test_c, y_pred_c, target_names=le.classes_))\n",
        "\n",
        "plot_history(history_crnn, title_prefix=\"[CRNN Log-Mel]\")\n",
        "plot_confusion(y_test_c, y_pred_c, labels=le.classes_, title=\"[CRNN Log-Mel] Matrice de confusion (normalisée)\")\n",
        "\n",
        "model_crnn.save(\"/content/drive/MyDrive/IA/Model Save/Model_CRNN_LogMel.keras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réseau CRNN avec Attention (Log-Mel)\n",
        "\n",
        "Le cinquième modèle enrichit le CRNN précédent par un mécanisme d’attention temporelle permettant au réseau de pondérer différemment chaque trame du signal selon sa pertinence.\n",
        "Cette approche vise à concentrer l’attention du modèle sur les segments les plus informatifs du chant.\n",
        "Les performances se stabilisent autour de 74 % d’accuracy test (macro-F1 ≈ 0.73), comparables au CNN 2D mais avec une meilleure robustesse aux variations temporelles.\n",
        "Le modèle démontre une bonne capacité de généralisation et une interprétabilité accrue via la visualisation des poids d’attention."
      ],
      "metadata": {
        "id": "GMXELVtJR4EQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_VKUwDFP0yv"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PIPELINE 5 — CRNN + Temporal Pooling Appris sur Log-Mel\n",
        "# ============================================================\n",
        "\n",
        "Xatt_list, yatt_list = [], []\n",
        "for fp, yi in tqdm(zip(label_df[\"filepath\"].values, y_all_ref),\n",
        "                   total=len(label_df), desc=\"Log-Mel (CRNN+Attention)\"):\n",
        "    try:\n",
        "        y_wav = load_fixed_mono(fp, sr=SR, target_dur=TARGET_DUR)\n",
        "        feat  = logmel_2d(y_wav, sr=SR, n_fft=N_FFT, hop=HOP, n_mels=N_MELS, norm=\"per_band\")\n",
        "        feat  = feat[..., np.newaxis]\n",
        "        Xatt_list.append(feat)\n",
        "        yatt_list.append(yi)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {fp} -> {e}\")\n",
        "\n",
        "X_att = np.stack(Xatt_list)\n",
        "y_att = np.array(yatt_list, dtype=int)\n",
        "print(\"CRNN+Att — Features:\", X_att.shape, \"| Labels:\", y_att.shape)\n",
        "\n",
        "X_train_a, X_val_a, X_test_a, y_train_a, y_val_a, y_test_a = slice_split(X_att, y_att)\n",
        "\n",
        "classes_unique = np.unique(y_train_a)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_unique, y=y_train_a)\n",
        "class_weight = {int(c): float(w) for c, w in zip(classes_unique, cw)}\n",
        "print(\"class_weight:\", class_weight)\n",
        "\n",
        "num_classes_att = len(np.unique(y_att))\n",
        "\n",
        "# Modèle CRNN + Attention\n",
        "def conv_block(x, filters, pool=(2,2), drop=0.0):\n",
        "    x = layers.Conv2D(filters, (3,3), padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D(pool)(x)\n",
        "    if drop and drop > 0:\n",
        "        x = layers.Dropout(drop)(x)\n",
        "    return x\n",
        "\n",
        "inp = layers.Input(shape=(X_train_a.shape[1], X_train_a.shape[2], 1))\n",
        "x = conv_block(inp,  32, pool=(2,2), drop=0.10)\n",
        "x = conv_block(x,    64, pool=(2,2), drop=0.10)\n",
        "x = conv_block(x,   128, pool=(2,1), drop=0.20)\n",
        "\n",
        "# (F', T', C) -> (T', F', C) -> (T', F'*C)\n",
        "x = layers.Permute((2,1,3))(x)\n",
        "x = layers.TimeDistributed(layers.Flatten())(x)\n",
        "\n",
        "# BiGRU temporel\n",
        "x = layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "\n",
        "# scores par trame\n",
        "w = layers.Dense(1, activation=\"tanh\")(x)         # (batch, T', 1)\n",
        "a = layers.Softmax(axis=1, name=\"att_weights\")(w) # (batch, T', 1)\n",
        "\n",
        "x = layers.Multiply()([x, a])                     # (batch, T', feat)\n",
        "x = layers.Lambda(lambda t: tf.reduce_sum(t, axis=1), name=\"att_pool\")(x)  # (batch, feat)\n",
        "\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "out = layers.Dense(num_classes_att, activation=\"softmax\")(x)\n",
        "\n",
        "model_crnn_att = keras.Model(inp, out)\n",
        "model_crnn_att.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                       loss=\"sparse_categorical_crossentropy\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "\n",
        "cbs_crnn_att = [\n",
        "    callbacks.EarlyStopping(patience=6, restore_best_weights=True),\n",
        "    callbacks.ReduceLROnPlateau(patience=3, factor=0.5),\n",
        "]\n",
        "\n",
        "history_crnn_att = model_crnn_att.fit(\n",
        "    X_train_a, y_train_a,\n",
        "    validation_data=(X_val_a, y_val_a),\n",
        "    epochs=40, batch_size=64, callbacks=cbs_crnn_att, verbose=1,\n",
        "    class_weight=class_weight\n",
        ")\n",
        "\n",
        "y_pred_a = model_crnn_att.predict(X_test_a, batch_size=64).argmax(axis=1)\n",
        "print(\"\\n[CRNN+Att] Test accuracy:\", (y_pred_a == y_test_a).mean())\n",
        "print(\"[CRNN+Att] Macro-F1:\", f1_score(y_test_a, y_pred_a, average=\"macro\"))\n",
        "print(classification_report(y_test_a, y_pred_a, target_names=le.classes_))\n",
        "\n",
        "plot_history(history_crnn_att, title_prefix=\"[CRNN+Attention Log-Mel]\")\n",
        "plot_confusion(y_test_a, y_pred_a, labels=le.classes_, title=\"[CRNN+Attention] Matrice de confusion (normalisée)\")\n",
        "\n",
        "model_crnn_att.save(\"/content/drive/MyDrive/IA/Model Save/Model_CRNN_Attention_LogMel.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AKCzwX13Q2K"
      },
      "outputs": [],
      "source": [
        "TEST_WAV = \"/content/drive/MyDrive/IA/Whistle/Sifflement.wav\"\n",
        "MODEL    = model_crnn\n",
        "\n",
        "y = load_fixed_mono(TEST_WAV, sr=SR, target_dur=3.0)\n",
        "LM = logmel_2d(y, sr=SR, n_fft=N_FFT, hop=HOP, n_mels=N_MELS, norm=\"per_band\")  # (F, T)\n",
        "X = LM[..., np.newaxis][np.newaxis, ...]  # (batch=1, F, T, C=1)\n",
        "\n",
        "proba = MODEL.predict(X, verbose=0)[0]\n",
        "\n",
        "top_idx = np.argsort(proba)[::-1]\n",
        "for i in range(min(5, len(proba))):\n",
        "    cls = le.classes_[top_idx[i]]\n",
        "    sc  = float(proba[top_idx[i]])\n",
        "    print(f\"{i+1}. {cls:12s} — {sc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion générale\n",
        "\n",
        "Plusieurs informations se dégagent à la fois sur le choix du traitement audio que sur la structure du modèle de classification :\n",
        "\n",
        "##1. Traitement du signal\n",
        "\n",
        "Les résultats montrent que la qualité des features extraites joue un rôle déterminant dans la performance finale :\n",
        "\n",
        "* Les MFCC vectorisés (MLP), bien que simples à mettre en œuvre, perdent la dimension temporelle du signal. Ils ne permettent donc pas au réseau d’exploiter la dynamique du chant.\n",
        "→ Résultat : environ 46 % d’accuracy, limité par une représentation trop compacte.\n",
        "\n",
        "* Les MFCC 2D (CNN) conservent la structure temps–fréquence. Le modèle parvient ainsi à détecter des motifs acoustiques typiques des espèces.\n",
        "→ Amélioration nette : environ 74 % d’accuracy, soit un gain de +28 points par rapport au MLP.\n",
        "\n",
        "* Les log-Mel spectrogrammes offrent une représentation plus perceptuellement cohérente (échelle quasi logarithmique comme l’oreille humaine).\n",
        "→ Le score atteint environ 69 %, légèrement inférieur au CNN MFCC, mais plus robuste sur certaines classes minoritaires.\n",
        "\n",
        "\n",
        "## 2. Architecture du modèle\n",
        "\n",
        "Le passage d’un CNN pur à une architecture hybride CNN + RNN (CRNN) constitue une avancée :\n",
        "\n",
        "* Le CRNN combine la détection locale des motifs fréquentiels (via les convolutions) et la modélisation temporelle à long terme (via GRU bidirectionnels).\n",
        "→ Il atteint environ 78 % d’accuracy et macro-F1 ≈ 0.78, confirmant sa capacité à capturer à la fois la texture spectrale et l’évolution temporelle du chant.\n",
        "\n",
        "* Le CRNN avec attention temporelle introduit une pondération adaptative sur les trames les plus pertinentes, améliorant la lisibilité et la stabilité du modèle.\n",
        "→ Les performances se stabilisent autour de 74 %, légèrement inférieures au CRNN pur mais avec une meilleure interprétabilité grâce aux cartes d’attention.\n",
        "\n",
        "##3. Analyse comparative\n",
        "\n",
        "| Modèle   | Représentation | Type de réseau          | Accuracy test | Macro-F1 | Points forts                                   |\n",
        "| -------- | -------------- | ----------------------- | ------------- | -------- | ---------------------------------------------- |\n",
        "| MLP      | MFCC 1D        | Dense                   | 0.46          | 0.45     | Simple, rapide, baseline                       |\n",
        "| CNN      | MFCC 2D        | Convolutif              | **0.74**      | **0.74** | Capture les motifs temps-fréquence             |\n",
        "| CNN      | Log-Mel 2D     | Convolutif              | 0.69          | 0.70     | Représentation perceptuelle                    |\n",
        "| CRNN     | Log-Mel 2D     | CNN + BiGRU             | **0.78**      | **0.78** | Meilleur compromis entre structure et séquence |\n",
        "| CRNN+Att | Log-Mel 2D     | CNN + BiGRU + Attention | 0.74          | 0.73     | Interprétable, stable                          |\n",
        "\n",
        "\n",
        "##4. Bilan final\n",
        "\n",
        "En résumé :\n",
        "\n",
        "* Le meilleur modèle global est le CRNN (CNN + BiGRU) entraîné sur les log-Mel spectrogrammes.\n",
        "\n",
        "* Il tire parti des caractéristiques spectrales locales et des relations temporelles longues, ce qui est un très bon point pour la reconnaissance de chants d’oiseaux.\n",
        "\n",
        "* L’ajout d’un mécanisme d’attention n’a pas amélioré la performance.\n",
        "\n",
        "* Ainsi, le meilleur traitement reste le log-Mel 2D, couplé à une architecture hybride convolutive et récurrente, confirmant que l’intégration du temps dans l’apprentissage est cruciale pour la classification sonore.\n",
        "\n"
      ],
      "metadata": {
        "id": "oPKaVBEGXJqw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}